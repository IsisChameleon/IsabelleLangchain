{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY \")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "APIFY_API_TOKEN = os.getenv(\"APIFY_API_TOKEN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries added for this project:  \n",
    ". matplotlib  \n",
    ". apify_client  \n",
    ". chromadb   https://docs.trychroma.com/getting-started\n",
    ". pypdf \n",
    ". arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIExSOwogICAgQS0tPiBCICYgQyAmIEQ7CiAgICBCLS0+IEEgJiBFOwogICAgQy0tPiBBICYgRTsKICAgIEQtLT4gQSAmIEU7CiAgICBFLS0+IEIgJiBDICYgRDsK\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise Mermaid diagram\n",
    "\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(graph):\n",
    "    graphbytes = graph.encode('ascii')\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode('ascii')\n",
    "    display(Image(url='https://mermaid.ink/img/' + base64_string))\n",
    "    \n",
    "visualize(\"\"\"\n",
    "graph LR;\n",
    "    A--> B & C & D;\n",
    "    B--> A & E;\n",
    "    C--> A & E;\n",
    "    D--> A & E;\n",
    "    E--> B & C & D;\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Mermaid documentation using Langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langchain.document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.base import Document\n",
    "from langchain.utilities import ApifyWrapper\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "\n",
    "url = 'https://mermaid.js.org/'\n",
    "\n",
    "loader = apify.call_actor(\n",
    "    actor_id='apify/website-content-crawler',\n",
    "    run_input={'startUrls': [{'url': url}]},\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item['text'] or '', \n",
    "        metadata={'source': item['url']}\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14257/2408102619.py:1: UserWarning: Callable dataset_mapping_function was excluded from schema since JSON schema has no equivalent type.\n",
      "  loader.schema()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'ApifyDatasetLoader',\n",
       " 'description': 'Logic for loading documents from Apify datasets.',\n",
       " 'type': 'object',\n",
       " 'properties': {'apify_client': {'title': 'Apify Client'},\n",
       "  'dataset_id': {'title': 'Dataset Id', 'type': 'string'}},\n",
       " 'required': ['dataset_id']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.schema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use loader to embed data and store in index store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "index = VectorstoreIndexCreator().from_loaders([loader]) #default openai embeddings with chromadb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the syntax for flowcharts?',\n",
       " 'answer': ' The syntax for flowcharts includes normal, thick, and dotted arrows, as well as special characters and subgraphs. It is also possible to declare multiple links and nodes in the same line, and to use new arrow types and multi-directional arrows.\\n',\n",
       " 'sources': 'https://mermaid.js.org/syntax/flowchart.html?id=flowcharts-basic-syntax, https://mermaid.js.org/syntax/flowchart.html?id=special-characters-that-break-syntax'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What is the syntax for flowcharts?'\n",
    "result = index.query_with_sources(query)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's turn this index into a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.vectorstore.as_retriever()\n",
    "# we change the number of document to return when the LLM queries the index store\n",
    "retriever.search_kwargs['k'] = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a chain to augment ChatGPT with this tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "mermaid_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Machine learning (ML) is a field devoted to understanding and building methods that let machines \"learn\" – that is, methods that leverage data to improve computer performance on some set of tasks.[1] Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.[3][4] A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[6][7] Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.[8][9] In its application across business problems, machine learning is also referred to as predictive analytics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Your job is to write the code to generate a colorful mermaid diagram describing the following text. \n",
    "Return only the code and make sure it has multiple colors\n",
    "\n",
    "TEXT: {text}\n",
    "\"\"\"\n",
    "result = mermaid_qa.run(query.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the code to generate a colorful mermaid diagram for the given text:\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "A[Machine learning (ML)] --> B(Understanding and building methods)\n",
      "A --> C(Leveraging data)\n",
      "B --> D(Improving computer performance)\n",
      "C --> D\n",
      "D --> E(Machine learning algorithms)\n",
      "E --> F(Building a model)\n",
      "F --> G(Training data)\n",
      "G --> H(Making predictions or decisions)\n",
      "H --> I(Without explicit programming)\n",
      "A --> J(Wide variety of applications)\n",
      "J --> K(Medicine)\n",
      "J --> L(Email filtering)\n",
      "J --> M(Speech recognition)\n",
      "J --> N(Agriculture)\n",
      "J --> O(Computer vision)\n",
      "J --> P(Difficult or unfeasible to develop conventional algorithms)\n",
      "K --> Q(Performing medical tasks)\n",
      "L --> Q\n",
      "M --> Q\n",
      "N --> Q\n",
      "O --> Q\n",
      "P --> Q\n",
      "Q --> R(Subfield of machine learning)\n",
      "R --> S(Computational statistics)\n",
      "R --> T(Mathematical optimization)\n",
      "R --> U(Application domains)\n",
      "T --> V(Predictions using computers)\n",
      "U --> V\n",
      "V --> W(Data mining)\n",
      "W --> X(Exploratory data analysis)\n",
      "X --> Y(Unsupervised learning)\n",
      "E --> Z(Neural networks)\n",
      "Z --> AA(Biological brain)\n",
      "AA --> AB(Machine learning implementations)\n",
      "AB --> AC(Business problems)\n",
      "AC --> AD(Predictive analytics)\n",
      "```\n",
      "\n",
      "Please note that the colors used in the diagram are not specified in the given text, so I have used default colors. You can customize the colors according to your preference.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFREOwpBW01hY2hpbmUgbGVhcm5pbmcgKE1MKV0gLS0+IEIoVW5kZXJzdGFuZGluZyBhbmQgYnVpbGRpbmcgbWV0aG9kcyk7CkEgLS0+IEMoTGV2ZXJhZ2luZyBkYXRhKTsKQiAtLT4gRChJbXByb3ZpbmcgY29tcHV0ZXIgcGVyZm9ybWFuY2UpOwpDIC0tPiBEOwpEIC0tPiBFKE1hY2hpbmUgbGVhcm5pbmcgYWxnb3JpdGhtcyk7CkUgLS0+IEYoQnVpbGRpbmcgYSBtb2RlbCk7CkYgLS0+IEcoVHJhaW5pbmcgZGF0YSk7CkcgLS0+IEgoTWFraW5nIHByZWRpY3Rpb25zIG9yIGRlY2lzaW9ucyk7CkggLS0+IEkoV2l0aG91dCBleHBsaWNpdCBwcm9ncmFtbWluZyk7CkEgLS0+IEooV2lkZSB2YXJpZXR5IG9mIGFwcGxpY2F0aW9ucyk7CkogLS0+IEsoTWVkaWNpbmUpOwpKIC0tPiBMKEVtYWlsIGZpbHRlcmluZyk7CkogLS0+IE0oU3BlZWNoIHJlY29nbml0aW9uKTsKSiAtLT4gTihBZ3JpY3VsdHVyZSk7CkogLS0+IE8oQ29tcHV0ZXIgdmlzaW9uKTsKSiAtLT4gUChEaWZmaWN1bHQgb3IgdW5mZWFzaWJsZSB0byBkZXZlbG9wIGNvbnZlbnRpb25hbCBhbGdvcml0aG1zKTsKSyAtLT4gUShQZXJmb3JtaW5nIG1lZGljYWwgdGFza3MpOwpMIC0tPiBROwpNIC0tPiBROwpOIC0tPiBROwpPIC0tPiBROwpQIC0tPiBROwpRIC0tPiBSKFN1YmZpZWxkIG9mIG1hY2hpbmUgbGVhcm5pbmcpOwpSIC0tPiBTKENvbXB1dGF0aW9uYWwgc3RhdGlzdGljcyk7ClIgLS0+IFQoTWF0aGVtYXRpY2FsIG9wdGltaXphdGlvbik7ClIgLS0+IFUoQXBwbGljYXRpb24gZG9tYWlucyk7ClQgLS0+IFYoUHJlZGljdGlvbnMgdXNpbmcgY29tcHV0ZXJzKTsKVSAtLT4gVjsKViAtLT4gVyhEYXRhIG1pbmluZyk7ClcgLS0+IFgoRXhwbG9yYXRvcnkgZGF0YSBhbmFseXNpcyk7ClggLS0+IFkoVW5zdXBlcnZpc2VkIGxlYXJuaW5nKTsKRSAtLT4gWihOZXVyYWwgbmV0d29ya3MpOwpaIC0tPiBBQShCaW9sb2dpY2FsIGJyYWluKTsKQUEgLS0+IEFCKE1hY2hpbmUgbGVhcm5pbmcgaW1wbGVtZW50YXRpb25zKTsKQUIgLS0+IEFDKEJ1c2luZXNzIHByb2JsZW1zKTsKQUMgLS0+IEFEKFByZWRpY3RpdmUgYW5hbHl0aWNzKTsK\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(\"\"\"\n",
    "graph TD;\n",
    "A[Machine learning (ML)] --> B(Understanding and building methods);\n",
    "A --> C(Leveraging data);\n",
    "B --> D(Improving computer performance);\n",
    "C --> D;\n",
    "D --> E(Machine learning algorithms);\n",
    "E --> F(Building a model);\n",
    "F --> G(Training data);\n",
    "G --> H(Making predictions or decisions);\n",
    "H --> I(Without explicit programming);\n",
    "A --> J(Wide variety of applications);\n",
    "J --> K(Medicine);\n",
    "J --> L(Email filtering);\n",
    "J --> M(Speech recognition);\n",
    "J --> N(Agriculture);\n",
    "J --> O(Computer vision);\n",
    "J --> P(Difficult or unfeasible to develop conventional algorithms);\n",
    "K --> Q(Performing medical tasks);\n",
    "L --> Q;\n",
    "M --> Q;\n",
    "N --> Q;\n",
    "O --> Q;\n",
    "P --> Q;\n",
    "Q --> R(Subfield of machine learning);\n",
    "R --> S(Computational statistics);\n",
    "R --> T(Mathematical optimization);\n",
    "R --> U(Application domains);\n",
    "T --> V(Predictions using computers);\n",
    "U --> V;\n",
    "V --> W(Data mining);\n",
    "W --> X(Exploratory data analysis);\n",
    "X --> Y(Unsupervised learning);\n",
    "E --> Z(Neural networks);\n",
    "Z --> AA(Biological brain);\n",
    "AA --> AB(Machine learning implementations);\n",
    "AB --> AC(Business problems);\n",
    "AC --> AD(Predictive analytics);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template='''Your job is to write the code to generate a colorful mermaid diagram describing the following text: {text}. \n",
    "Return only the code and make sure it has multiple colors'''\n",
    ")\n",
    "\n",
    "result = mermaid_qa.run(prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To generate a colorful mermaid diagram describing the given text, you can use the following code:\n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "A(Machine learning) --> B(Understanding and building methods);\n",
      "A --> C(Leveraging data);\n",
      "A --> D(Improving computer performance);\n",
      "B --> E(Methods that let machines \"learn\");\n",
      "C --> F(Improving computer performance);\n",
      "D --> F(Improving computer performance);\n",
      "E --> G(Building a model based on sample data);\n",
      "G --> H(Making predictions or decisions);\n",
      "H --> I(Without explicit programming);\n",
      "F --> J(Used in various applications);\n",
      "J --> K(Medicine, email filtering, speech recognition, agriculture, computer vision);\n",
      "K --> L(Difficult or unfeasible to develop conventional algorithms);\n",
      "M(Machine learning subset);\n",
      "M --> N(Computational statistics);\n",
      "N --> O(Making predictions using computers);\n",
      "P(Mathematical optimization);\n",
      "P --> Q(Methods, theory, application domains);\n",
      "R(Data mining);\n",
      "R --> S(Exploratory data analysis);\n",
      "S --> T(Unsupervised learning);\n",
      "U(Biological brain mimicking);\n",
      "U --> V(Data and neural networks);\n",
      "W(Predictive analytics);\n",
      "\n",
      "style A fill:#FFB6C1, stroke:#FF1493;\n",
      "style B fill:#FFD700, stroke:#FFA500;\n",
      "style C fill:#00BFFF, stroke:#1E90FF;\n",
      "style D fill:#ADFF2F, stroke:#32CD32;\n",
      "style E fill:#FF69B4, stroke:#FF00FF;\n",
      "style F fill:#FF4500, stroke:#FF8C00;\n",
      "style G fill:#008000, stroke:#006400;\n",
      "style H fill:#FFA07A, stroke:#FF7F50;\n",
      "style I fill:#00FFFF, stroke:#00CED1;\n",
      "style J fill:#FFFF00, stroke:#FFD700;\n",
      "style K fill:#9400D3, stroke:#8A2BE2;\n",
      "style L fill:#00BFFF, stroke:#1E90FF;\n",
      "style M fill:#FF6347, stroke:#FF4500;\n",
      "style N fill:#7B68EE, stroke:#6A5ACD;\n",
      "style O fill:#FFA500, stroke:#FF8C00;\n",
      "style P fill:#00FF00, stroke:#006400;\n",
      "style Q fill:#FF69B4, stroke:#FF00FF;\n",
      "style R fill:#4169E1, stroke:#0000CD;\n",
      "style S fill:#00BFFF, stroke:#1E90FF;\n",
      "style T fill:#87CEEB, stroke:#00CED1;\n",
      "style U fill:#FF4500, stroke:#FF8C00;\n",
      "style V fill:#FF1493, stroke:#FF00FF;\n",
      "style W fill:#FFFF00, stroke:#FFD700;\n",
      "```\n",
      "\n",
      "This code assigns different colors to each node in the diagram, creating a colorful representation of the given text.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./attention.pdf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "paper = next(arxiv.Search(id_list=['1706.03762']).results())\n",
    "paper.download_pdf(filename='attention.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Attention Is All You Need\\nAshish Vaswani\\x03\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\x03\\nGoogle Brain\\nnoam@google.comNiki Parmar\\x03\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\x03\\nGoogle Research\\nusz@google.com\\nLlion Jones\\x03\\nGoogle Research\\nllion@google.comAidan N. Gomez\\x03y\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser\\x03\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\x03z\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nyWork performed while at Google Brain.\\nzWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017' metadata={'source': 'attention.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('attention.pdf')\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "concept_chain = load_qa_chain(\n",
    "    llm=llm, \n",
    "    chain_type='map_reduce'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformer||Attention mechanisms||Encoder-decoder architecture||Self-attention||Multi-Head Attention'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "The text provided as context is an extract of an academic paper in the field of machine learning.\n",
    "Your job is to find the main concepts that are explained in this paper and return those concepts as a list, all on one line separated by the characters '||'. Order that list from most essential concept for the paper to least essential. Limit the list to the 5 most important concepts\n",
    "\n",
    "Example: concept 1||concept 2||concept 3 \n",
    "\"\"\"\n",
    "\n",
    "result = concept_chain({'input_documents': pages, 'question': query})\n",
    "\n",
    "result['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformer',\n",
       " 'Attention mechanisms',\n",
       " 'Encoder-decoder architecture',\n",
       " 'Self-attention',\n",
       " 'Multi-Head Attention']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_list = result['output_text'].split('||')\n",
    "concept_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_chain = load_qa_chain(\n",
    "    llm=llm, \n",
    "    chain_type='map_reduce'\n",
    ")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query = \"\"\"\n",
    "The text provided as context is an extract from an academic paper in machine learning.\n",
    "\n",
    "Your job is to use that context to explain the following concept. \n",
    "The answer must be self-contained so you cannot refer to the article in the answer.\n",
    "Imagine you are a Computer Science Professor teaching at the university.\n",
    "Respond only with an explanation of the concept:\n",
    "\n",
    "\n",
    "CONCEPT: {concept} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The concept of a Transformer refers to a type of neural network architecture that has revolutionized the field of natural language processing (NLP). It is a deep learning model specifically designed for handling sequential data, such as sentences or documents. Unlike traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), Transformers rely on a self-attention mechanism to capture the relationships between different words or tokens in a sequence.\\n\\nIn a Transformer, the input sequence is first encoded into a set of embeddings. These embeddings are then processed through multiple layers of self-attention and feed-forward neural networks. The self-attention mechanism allows each word in the sequence to attend to all other words, capturing the importance and relevance of each word in the context of the entire sequence.\\n\\nBy leveraging self-attention, Transformers can model dependencies between words regardless of their position in the sequence, making them highly effective in capturing long-range dependencies. This is in contrast to RNNs, which suffer from vanishing or exploding gradients when dealing with long sequences.\\n\\nTransformers have proven to be highly successful in various NLP tasks, such as machine translation, text summarization, and language understanding. They have become the state-of-the-art architecture for many NLP tasks due to their ability to model long-range dependencies and their computational efficiency in parallel processing.\\n\\nIn summary, Transformers are a type of neural network architecture that uses self-attention mechanisms to capture relationships between words in a sequence. They have revolutionized the field of NLP and have achieved impressive results in various tasks by effectively capturing long-range dependencies.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations = []\n",
    "\n",
    "for concept in concept_list:\n",
    "    \n",
    "    result = explain_chain({\n",
    "        'input_documents': pages, \n",
    "        'question': query.format(concept=concept)\n",
    "    })\n",
    "    \n",
    "    explanations.append({\n",
    "        'concept': concept,\n",
    "        'explanation': result['output_text']  \n",
    "    })\n",
    "\n",
    "explanations[0]['explanation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concept of a Transformer refers to a type of neural network architecture that has revolutionized the field of natural language processing (NLP). It is a deep learning model specifically designed for handling sequential data, such as sentences or documents. Unlike traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), Transformers rely on a self-attention mechanism to capture the relationships between different words or tokens in a sequence.\n",
      "\n",
      "In a Transformer, the input sequence is first encoded into a set of embeddings. These embeddings are then processed through multiple layers of self-attention and feed-forward neural networks. The self-attention mechanism allows each word in the sequence to attend to all other words, capturing the importance and relevance of each word in the context of the entire sequence.\n",
      "\n",
      "By leveraging self-attention, Transformers can model dependencies between words regardless of their position in the sequence, making them highly effective in capturing long-range dependencies. This is in contrast to RNNs, which suffer from vanishing or exploding gradients when dealing with long sequences.\n",
      "\n",
      "Transformers have proven to be highly successful in various NLP tasks, such as machine translation, text summarization, and language understanding. They have become the state-of-the-art architecture for many NLP tasks due to their ability to model long-range dependencies and their computational efficiency in parallel processing.\n",
      "\n",
      "In summary, Transformers are a type of neural network architecture that uses self-attention mechanisms to capture relationships between words in a sequence. They have revolutionized the field of NLP and have achieved impressive results in various tasks by effectively capturing long-range dependencies.\n"
     ]
    }
   ],
   "source": [
    "print(explanations[0]['explanation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
